<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Python深度学习之PyTorch基础教程 | 江湖是你画中亦是你</title><meta name="keywords" content="PyTorch"><meta name="author" content="Xiaotangsmiles"><meta name="copyright" content="Xiaotangsmiles"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="⛄前言 PyTorch是一个基于Torch的Python开源机器学习(深度学习)框架，由Facebook的人工智能研究院开发，不仅能够实现强大的GPU加速，还支持动态神经网络，使得研究人员和开发人员能够轻松构建和训练复杂的深度学习模型。与TensorFlow等其他框架相比，Pytorch的主要优势在于其简单易用的接口、高效的性能和强大的生态系统。PyTorch的主要特点和功能：  基本概念：">
<meta property="og:type" content="article">
<meta property="og:title" content="Python深度学习之PyTorch基础教程">
<meta property="og:url" content="https://heartlovelife.github.io/2024/06/10/DL-PyTorch-Basic/index.html">
<meta property="og:site_name" content="江湖是你画中亦是你">
<meta property="og:description" content="⛄前言 PyTorch是一个基于Torch的Python开源机器学习(深度学习)框架，由Facebook的人工智能研究院开发，不仅能够实现强大的GPU加速，还支持动态神经网络，使得研究人员和开发人员能够轻松构建和训练复杂的深度学习模型。与TensorFlow等其他框架相比，Pytorch的主要优势在于其简单易用的接口、高效的性能和强大的生态系统。PyTorch的主要特点和功能：  基本概念：">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2024/06/10/KdBQXSmlCgFOWR1.jpg">
<meta property="article:published_time" content="2024-06-10T14:00:05.000Z">
<meta property="article:modified_time" content="2024-06-10T07:06:40.791Z">
<meta property="article:author" content="Xiaotangsmiles">
<meta property="article:tag" content="PyTorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2024/06/10/KdBQXSmlCgFOWR1.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://heartlovelife.github.io/2024/06/10/DL-PyTorch-Basic/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":250},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Python深度学习之PyTorch基础教程',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-06-10 15:06:40'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/loading1.gif" data-original="/img/touxiang.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">27</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">27</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">11</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/photos/"><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><span> 影视</span></a></li><li><a class="site-page child" href="/shuoshuo/"><span> 分享</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s2.loli.net/2024/06/10/KdBQXSmlCgFOWR1.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">江湖是你画中亦是你</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/photos/"><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><span> 影视</span></a></li><li><a class="site-page child" href="/shuoshuo/"><span> 分享</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><span> 关于笔者</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Python深度学习之PyTorch基础教程</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-06-10T14:00:05.000Z" title="发表于 2024-06-10 22:00:05">2024-06-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-06-10T07:06:40.791Z" title="更新于 2024-06-10 15:06:40">2024-06-10</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Python/">Python</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">12.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>48分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Python深度学习之PyTorch基础教程"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="前言">⛄前言</h2>
<p>PyTorch是一个基于Torch的Python开源机器学习(深度学习)框架，由Facebook的人工智能研究院开发，不仅能够实现强大的GPU加速，还支持动态神经网络，使得研究人员和开发人员能够轻松构建和训练复杂的深度学习模型。与TensorFlow等其他框架相比，Pytorch的主要优势在于其简单易用的接口、高效的性能和强大的生态系统。PyTorch的主要特点和功能：</p>
<ul>
<li><strong>基本概念</strong>：PyTorch的基本概念包括张量(Tensor)、自动求导(Autograd)和动态计算图(Dynamic Computation Graph)。张量是PyTorch最基本的数据类型，类似于多维数组，用于存储和处理大规模的数值数据，并支持各种数学运算和操作。自动求导是PyTorch的一个重要特性，它能自动计算张量的梯度，有助于深度学习模型的训练。动态计算图则使得PyTorch的计算过程可以灵活地进行构建和修改，有助于开发复杂的神经网络模型。</li>
<li><strong>基本使用方法</strong>：PyTorch提供了丰富的API和工具，使得用户可以方便地创建和操作张量，进行基本的数学运算，以及构建和训练神经网络模型。例如，用户可以轻松地创建一个矩阵，进行加法操作，改变矩阵的维度，以及与NumPy进行协同操作等。</li>
<li><strong>软件特色</strong>：PyTorch由多个库组成，包括Tensor库、自动分化库、神经网络库、多处理库和实用函数库等。这些库提供了丰富的功能和工具，使得PyTorch既可以作为Numpy的替代品，也可以作为深度学习研究平台，提供最佳的灵活性和速度。</li>
<li><strong>生态系统组件</strong>：torchvision一个用于计算机视觉任务的库，包含常见数据集、预训练模型和工具；torchtext一个用于文本处理任务的库，支持文本清洗、分词等功能；torch.nn一个用于构建神经网络的模块，提供了各种层、损失函数和优化器；torch.distributed一个用于分布式训练的库，可以轻松实现多机多卡训练。</li>
<li><strong>学习资源</strong>：PyTorch官方文档提供了详细的教程和API文档，适合初学者入门和深入学习。此外，PyTorch中文网、GitHub上的开源项目以及博客、论坛和在线社区等也提供了丰富的教程、解答和讨论，有助于用户更好地学习和使用PyTorch。</li>
<li><strong>应用场景</strong>：PyTorch的应用场景非常广泛，包括图像识别、自然语言处理、计算机视觉等领域。例如，在图像识别领域，PyTorch可以用于训练图像分类器；在自然语言处理领域，PyTorch可以用于训练文本分类器；在计算机视觉领域，PyTorch可以用于实现计算机视觉推理系统等。</li>
</ul>
<p>总的来说，PyTorch是一个功能强大、灵活易用的深度学习平台，适用于各种人工智能研究和应用场景。PyTorch是一个基于Python的科学计算包，主要定位两类人群：</p>
<ul>
<li><p>NumPy的替代品，可以利用GPU的性能进行计算；</p></li>
<li><p>深度学习研究平台拥有足够的灵活性和速度。</p></li>
</ul>
<h2 id="pytorch基础">⛄PyTorch基础</h2>
<h3 id="numpy基础">👀Numpy基础</h3>
<p>在机器学习和深度学习中，图像、声音、文本等输入数据最终都要转换为数组或矩阵。如何有效地进行数组和矩阵的运算？这就需要充分利用Numpy。Numpy是数据科学的通用语言，而且与PyTorch关系非常密切，它是科学计算、深度学习的基石。尤其对PyTorch而言，其重要性更加明显。PyTorch中的Tensor与Numpy非常相似，它们之间可以非常方便地进行转换，掌握Numpy是学好PyTorch的重要基础。</p>
<p>Numpy(Numerical Python)提供了两种基本的对象：ndarray(N-dimensional Array Object)和ufunc(Universal Function Object)。ndarray是存储单一数据类型的多维数组，而ufunc则是能够对数组进行处理的函数。</p>
<h4 id="numpy数组">👁Numpy数组</h4>
<p><strong>（1）从已有数据中创建数组</strong></p>
<p>通过直接对Python的基础数据类型(如列表、元组等)进行转换来生成ndarray：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 将列表转换成ndarray</span></span><br><span class="line">list1 = [<span class="number">3.5</span>, <span class="number">2.5</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">nd1 = np.array(list1)</span><br><span class="line"><span class="built_in">print</span>(nd1)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(nd1))</span><br><span class="line"><span class="comment"># 嵌套列表可以转换成多维ndarray</span></span><br><span class="line">list2 = [[<span class="number">3.5</span>, <span class="number">2.5</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]]</span><br><span class="line">nd2 = np.array(list2)</span><br><span class="line"><span class="built_in">print</span>(nd2)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(nd2))</span><br><span class="line"><span class="comment"># 将元组转换成ndarray</span></span><br><span class="line">list3 = (<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line">nd3 = np.array(list3)</span><br><span class="line"><span class="built_in">print</span>(nd3)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(nd3))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">[<span class="number">3.5</span> <span class="number">2.5</span> <span class="number">0.</span>  <span class="number">1.</span>  <span class="number">2.</span> ]</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;numpy.ndarray&#x27;</span>&gt;</span><br><span class="line">[[<span class="number">3.5</span> <span class="number">2.5</span> <span class="number">0.</span>  <span class="number">1.</span>  <span class="number">2.</span> ]</span><br><span class="line"> [<span class="number">1.</span>  <span class="number">2.</span>  <span class="number">3.</span>  <span class="number">4.</span>  <span class="number">5.</span> ]]</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;numpy.ndarray&#x27;</span>&gt;</span><br><span class="line">[<span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;numpy.ndarray&#x27;</span>&gt;</span><br></pre></td></tr></table></figure>
<p><strong>（2）利用random模块生成数组</strong></p>
<p>在深度学习中，我们需要对一些参数进行初始化，为了更有效地训练模型，提高模型的性能，有些初始化还需要满足一定的条件，如满足正态分布或均匀分布等。np.random模块常用函数：</p>
<table>
<thead>
<tr class="header">
<th>函数</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>np.random.random</td>
<td>生成0到1之间的随机数</td>
</tr>
<tr class="even">
<td>np.random.uniform</td>
<td>生成均匀分布的随机数</td>
</tr>
<tr class="odd">
<td>np.random.randn</td>
<td>生成标准正态的随机数</td>
</tr>
<tr class="even">
<td>np.random.randint</td>
<td>生成随机的整数</td>
</tr>
<tr class="odd">
<td>np.random.normal</td>
<td>生成正态分布</td>
</tr>
<tr class="even">
<td>np.random.shuffle</td>
<td>随机打乱顺序</td>
</tr>
<tr class="odd">
<td>np.random.seed</td>
<td>设置随机数种子</td>
</tr>
<tr class="even">
<td>np.random.random_sample</td>
<td>生成随机的浮点数</td>
</tr>
<tr class="odd">
<td>...</td>
<td>...</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">nd = np.random.random([<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(nd)</span><br><span class="line"><span class="built_in">print</span>(nd.shape)</span><br><span class="line">np.random.shuffle(nd)</span><br><span class="line"><span class="built_in">print</span>(nd)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">[[<span class="number">0.96651734</span> <span class="number">0.71544924</span> <span class="number">0.70717421</span>]</span><br><span class="line"> [<span class="number">0.92522016</span> <span class="number">0.99484174</span> <span class="number">0.00550428</span>]</span><br><span class="line"> [<span class="number">0.36218919</span> <span class="number">0.11263693</span> <span class="number">0.1816698</span> ]]</span><br><span class="line">(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">[[<span class="number">0.92522016</span> <span class="number">0.99484174</span> <span class="number">0.00550428</span>]</span><br><span class="line"> [<span class="number">0.96651734</span> <span class="number">0.71544924</span> <span class="number">0.70717421</span>]</span><br><span class="line"> [<span class="number">0.36218919</span> <span class="number">0.11263693</span> <span class="number">0.1816698</span> ]]</span><br></pre></td></tr></table></figure>
<p><strong>（3）创建特定形状的多维数组</strong></p>
<p>参数初始化时，有时需要生成一些特殊矩阵，如全是0或1的数组或矩阵。</p>
<table>
<thead>
<tr class="header">
<th>函数</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>np.zeros((m,n))</td>
<td>创建m×n的元素全为0的数组</td>
</tr>
<tr class="even">
<td>np.ones((m,n))</td>
<td>创建m×n的元素全为1的数组</td>
</tr>
<tr class="odd">
<td>np.empty((m,n))</td>
<td>创建m×n的空数组，空数据中的值并不为0，而是未初始化的垃圾值</td>
</tr>
<tr class="even">
<td>np.zeros_like(ndarr)</td>
<td>以ndarr相同维度创建元素全为0数组</td>
</tr>
<tr class="odd">
<td>np.ones_like(ndarr)</td>
<td>以ndarr相同维度创建元素全为1数组</td>
</tr>
<tr class="even">
<td>np.empty_like(ndarr)</td>
<td>以ndarr相同维度创建空数组</td>
</tr>
<tr class="odd">
<td>np.eye(m)</td>
<td>该函数用于创建一个m×m的矩阵，对角线为1，其余为0</td>
</tr>
<tr class="even">
<td>np.full((m,n), value)</td>
<td>创建m×n的元素全为value的数组，value为指定值</td>
</tr>
<tr class="odd">
<td>...</td>
<td>...</td>
</tr>
</tbody>
</table>
<p><strong>（4）利用arange、linspace函数生成数组</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># numpy模块中的arange、linspace函数</span></span><br><span class="line"><span class="comment"># start与stop用来指定范围，step用来设定步长。在生成一个ndarray时，start默认为0，步长step可为小数。Python有个内置函数range，其功能与此类似。</span></span><br><span class="line">np.arange([start,] stop[,step,], dtype=<span class="literal">None</span>)</span><br><span class="line"><span class="comment"># linspace可以根据输入的指定数据范围以及等份数量，自动生成一个线性等分向量，其中endpoint（包含终点）默认为True，等分数量num默认为50。如果将retstep设置为True，则会返回一个带步长的ndarray。</span></span><br><span class="line">np.linspace(start, stop, num=<span class="number">50</span>, endpoint=<span class="literal">True</span>, retstep=<span class="literal">False</span>, dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<h4 id="获取元素">👁获取元素</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">nd = np.random.random([<span class="number">10</span>])</span><br><span class="line"><span class="built_in">print</span>(nd)</span><br><span class="line"><span class="comment"># 获取指定位置的数据，获取第4个元素</span></span><br><span class="line"><span class="built_in">print</span>(nd[<span class="number">3</span>])</span><br><span class="line"><span class="comment"># 截取一段数据</span></span><br><span class="line"><span class="built_in">print</span>(nd[<span class="number">3</span>:<span class="number">6</span>])</span><br><span class="line"><span class="comment"># 截取固定间隔数据</span></span><br><span class="line"><span class="built_in">print</span>(nd[<span class="number">1</span>:<span class="number">6</span>:<span class="number">2</span>])</span><br><span class="line"><span class="comment"># 倒序取数</span></span><br><span class="line"><span class="built_in">print</span>(nd[::-<span class="number">2</span>])</span><br><span class="line"><span class="comment"># 截取一个多维数组的一个区域内数据</span></span><br><span class="line">nd1 = np.arange(<span class="number">25</span>).reshape([<span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line"><span class="built_in">print</span>(nd1)</span><br><span class="line"><span class="built_in">print</span>(nd1[<span class="number">1</span>:<span class="number">3</span>, <span class="number">1</span>:<span class="number">3</span>])</span><br><span class="line"><span class="comment"># 截取一个多维数组中，数值在一个值域之内的数据</span></span><br><span class="line"><span class="built_in">print</span>(nd1[(nd1 &gt; <span class="number">3</span>) &amp; (nd1 &lt; <span class="number">10</span>)])</span><br><span class="line"><span class="comment"># 截取多维数组中，指定的行,如读取第2,3行</span></span><br><span class="line"><span class="built_in">print</span>(nd1[[<span class="number">1</span>, <span class="number">2</span>]])  <span class="comment"># 或nd12[1:3,:]</span></span><br><span class="line"><span class="comment"># 截取多维数组中，指定的列,如读取第2,3列</span></span><br><span class="line"><span class="built_in">print</span>(nd1[:, <span class="number">1</span>:<span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<p>获取数组中的部分元素除了通过指定索引标签来实现外，还可以通过使用一些函数来实现，如通过random.choice函数从指定的样本中随机抽取数据。</p>
<h4 id="numpy的算术运算">👁Numpy的算术运算</h4>
<p>在机器学习和深度学习中，涉及大量的数组或矩阵运算，我们将重点介绍两种常用的运算。</p>
<ul>
<li><p>一种是对应元素相乘，又称为逐元乘法(Element-Wise Product)，运算符为np.multiply()或*。</p></li>
<li><p>一种是点积或内积元素，运算符为np.dot()。</p></li>
</ul>
<p><strong>（1）对应元素相乘</strong></p>
<p>对应元素相乘(Element-Wise Product)是两个矩阵中对应元素乘积。np.multiply函数用于数组或矩阵对应元素相乘，输出与相乘数组或矩阵的大小一致，其格式如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 格式</span></span><br><span class="line"><span class="comment"># x1、x2之间的对应元素相乘遵守广播规则</span></span><br><span class="line">numpy.multiply(x1, x2, /, out=<span class="literal">None</span>, *, where=<span class="literal">True</span>, casting=<span class="string">&#x27;same_kind&#x27;</span>, order=<span class="string">&#x27;K&#x27;</span>, dtype=<span class="literal">None</span>, subok=<span class="literal">True</span>[, signature, extobj])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A = np.array([[<span class="number">1</span>, <span class="number">2</span>], [-<span class="number">1</span>, <span class="number">4</span>]])</span><br><span class="line">B = np.array([[<span class="number">2</span>, <span class="number">0</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="built_in">print</span>(A*B)</span><br><span class="line"><span class="built_in">print</span>(np.multiply(A, B))</span><br><span class="line"><span class="comment"># Numpy数组不仅可以和数组进行对应元素相乘，还可以和单一数值（或称为标量）进行运算。运算时，Numpy数组中的每个元素都和标量进行运算，其间会用到广播机制</span></span><br><span class="line"><span class="built_in">print</span>(A*<span class="number">2.0</span>)</span><br><span class="line"><span class="built_in">print</span>(A/<span class="number">2.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">[[ <span class="number">2</span>  <span class="number">0</span>]</span><br><span class="line"> [-<span class="number">3</span> <span class="number">16</span>]]</span><br><span class="line">[[ <span class="number">2</span>  <span class="number">0</span>]</span><br><span class="line"> [-<span class="number">3</span> <span class="number">16</span>]]</span><br><span class="line">[[ <span class="number">2.</span>  <span class="number">4.</span>]</span><br><span class="line"> [-<span class="number">2.</span>  <span class="number">8.</span>]]</span><br><span class="line">[[ <span class="number">0.5</span>  <span class="number">1.</span> ]</span><br><span class="line"> [-<span class="number">0.5</span>  <span class="number">2.</span> ]]</span><br></pre></td></tr></table></figure>
<p>由此，推而广之，数组通过一些激活函数后，输出与输入形状一致。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">X = np.random.rand(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.maximum(<span class="number">0</span>, x)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.exp(x) / np.<span class="built_in">sum</span>(np.exp(x))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入参数X的形状：&quot;</span>, X.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;激活函数softmoid输出形状：&quot;</span>, softmoid(X).shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;激活函数relu输出形状：&quot;</span>, relu(X).shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;激活函数softmax输出形状：&quot;</span>, softmax(X).shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">输入参数X的形状： (<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">激活函数softmoid输出形状： (<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">激活函数relu输出形状： (<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">激活函数softmax输出形状： (<span class="number">2</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p><strong>（2）点积运算</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 格式</span></span><br><span class="line">numpy.dot(a, b, out=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例</span></span><br><span class="line"><span class="comment"># 矩阵X1和矩阵X2进行点积运算，其中X1和X2对应维度（即X1的第2个维度与X2的第1个维度）的元素个数必须保持一致。此外，矩阵X3的形状是由矩阵X1的行数与矩阵X2的列数构成的。</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">X1 = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">X2 = np.array([[<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>], [<span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]])</span><br><span class="line">X3 = np.dot(X1, X2)</span><br><span class="line"><span class="built_in">print</span>(X3)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">[[<span class="number">21</span> <span class="number">24</span> <span class="number">27</span>]</span><br><span class="line"> [<span class="number">47</span> <span class="number">54</span> <span class="number">61</span>]]</span><br></pre></td></tr></table></figure>
<h4 id="数组变形">👁数组变形</h4>
<p>在机器学习及深度学习的任务中，通常需要将处理好的数据以模型能接收的格式输入给模型，然后由模型通过一系列的运算，最终返回一个处理结果。然而，由于不同模型所接收的输入格式不一样，往往需要先对其进行一系列的变形和运算，从而将数据处理成符合模型要求的格式。在矩阵或者数组的运算中，经常会遇到需要把多个向量或矩阵按某轴方向合并，或展平（如在卷积或循环神经网络中，在全连接层之前，需要把矩阵展平）的情况。</p>
<p><strong>（1）更改数组的形状</strong></p>
<p>修改指定数组的形状是Numpy中最常见的操作之一，常见函数：</p>
<table>
<thead>
<tr class="header">
<th>函数</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>arr.reshape</td>
<td>重新将向量arr维度进行改变，不修改向量本身</td>
</tr>
<tr class="even">
<td>arr.resize</td>
<td>重新将向量arr维度进行改变，修改向量本身</td>
</tr>
<tr class="odd">
<td>arr.T</td>
<td>对向量arr进行转置</td>
</tr>
<tr class="even">
<td>arr.ravel</td>
<td>对向量arr进行展平，即将多维数组变成1维数组，不会产生原数组的副本</td>
</tr>
<tr class="odd">
<td>arr.flatten</td>
<td>对向量arr进行展平，即将多维数组变成1维数组，返回原数组的副本</td>
</tr>
<tr class="even">
<td>arr.squeeze</td>
<td>只能对维数为1的维度降维。对多维数组使用时不会报错，但是不会产生任何影响</td>
</tr>
<tr class="odd">
<td>arr.transpose</td>
<td>对高维矩阵进行轴对换</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">arr = np.arange(<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(arr)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span> * <span class="number">20</span>)</span><br><span class="line"><span class="comment"># (1)reshape改变向量的维度（不修改向量本身）：</span></span><br><span class="line"><span class="comment"># 将向量 arr 维度变换为2行5列</span></span><br><span class="line"><span class="built_in">print</span>(arr.reshape(<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment"># 指定维度时可以只指定行数或列数, 其他用-1代替</span></span><br><span class="line"><span class="built_in">print</span>(arr.reshape(<span class="number">5</span>, -<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(arr.reshape(-<span class="number">1</span>, <span class="number">5</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span> * <span class="number">20</span>)</span><br><span class="line"><span class="comment"># (2)resize改变向量的维度（修改向量本身）</span></span><br><span class="line"><span class="comment"># 将向量 arr 维度变换为2行5列</span></span><br><span class="line">arr.resize(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(arr)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span> * <span class="number">20</span>)</span><br><span class="line"><span class="comment"># (3)T向量转置</span></span><br><span class="line"><span class="comment"># 将向量arr进行转置为5行2列</span></span><br><span class="line"><span class="built_in">print</span>(arr.T)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span> * <span class="number">20</span>)</span><br><span class="line"><span class="comment"># (4)ravel向量展平</span></span><br><span class="line"><span class="comment"># 按照列优先，展平</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;按照列优先，展平&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(arr.ravel(<span class="string">&#x27;F&#x27;</span>))</span><br><span class="line"><span class="comment"># 按照行优先，展平</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;按照行优先，展平&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(arr.ravel())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span> * <span class="number">20</span>)</span><br><span class="line"><span class="comment"># (5)flatten把矩阵转换为向量，这种需求经常出现在卷积网络与全连接层之间。</span></span><br><span class="line"><span class="built_in">print</span>(arr.flatten())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span> * <span class="number">20</span>)</span><br><span class="line"><span class="comment"># (6)squeeze这是一个主要用来降维的函数，把矩阵中含1的维度去掉。</span></span><br><span class="line">arr1 = np.arange(<span class="number">3</span>).reshape(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(arr1.shape)  <span class="comment"># (3,1)</span></span><br><span class="line"><span class="built_in">print</span>(arr1.squeeze().shape)  <span class="comment"># (3,)</span></span><br><span class="line">arr2 = np.arange(<span class="number">6</span>).reshape(<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(arr2.shape)  <span class="comment"># (3, 1, 2, 1)</span></span><br><span class="line"><span class="built_in">print</span>(arr2.squeeze().shape)  <span class="comment"># (3, 2)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span> * <span class="number">20</span>)</span><br><span class="line"><span class="comment"># (7)transpose对高维矩阵进行轴对换，在深度学习中经常使用，比如把图片中表示颜色顺序的RGB改为GBR。</span></span><br><span class="line">arr3 = np.arange(<span class="number">24</span>).reshape(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(arr3.shape)  <span class="comment"># (2, 3, 4)</span></span><br><span class="line"><span class="built_in">print</span>(arr3.transpose(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).shape)  <span class="comment"># (3, 4, 2)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">[<span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">9</span>]</span><br><span class="line">********************</span><br><span class="line">[[<span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span>]</span><br><span class="line"> [<span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">9</span>]]</span><br><span class="line">[[<span class="number">0</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span>]</span><br><span class="line"> [<span class="number">6</span> <span class="number">7</span>]</span><br><span class="line"> [<span class="number">8</span> <span class="number">9</span>]]</span><br><span class="line">[[<span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span>]</span><br><span class="line"> [<span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">9</span>]]</span><br><span class="line">********************</span><br><span class="line">[[<span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span>]</span><br><span class="line"> [<span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">9</span>]]</span><br><span class="line">********************</span><br><span class="line">[[<span class="number">0</span> <span class="number">5</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">6</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">7</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">8</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">9</span>]]</span><br><span class="line">********************</span><br><span class="line">按照列优先，展平</span><br><span class="line">[<span class="number">0</span> <span class="number">5</span> <span class="number">1</span> <span class="number">6</span> <span class="number">2</span> <span class="number">7</span> <span class="number">3</span> <span class="number">8</span> <span class="number">4</span> <span class="number">9</span>]</span><br><span class="line">按照行优先，展平</span><br><span class="line">[<span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">9</span>]</span><br><span class="line">********************</span><br><span class="line">[<span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">9</span>]</span><br><span class="line">********************</span><br><span class="line">(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">(<span class="number">3</span>,)</span><br><span class="line">(<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">********************</span><br><span class="line">(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">(<span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p><strong>（2）合并数组</strong></p>
<p>合并数组也是最常见的操作之一，常见的用于数组或向量合并的方法。</p>
<table>
<thead>
<tr class="header">
<th>函数</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>np.append</td>
<td>合并一维或多维数组，内存占用大</td>
</tr>
<tr class="even">
<td>np.concatenate</td>
<td>沿指定轴连接数组或矩阵，没有内存问题</td>
</tr>
<tr class="odd">
<td>np.stack</td>
<td>沿指定轴堆叠数组或矩阵，沿着新的轴加入一系列数组</td>
</tr>
<tr class="even">
<td>np.hstack</td>
<td>按水平方向（列顺序）堆叠数组构成一个新的数组，堆叠的数组需要具有相同的维度</td>
</tr>
<tr class="odd">
<td>np.vstack</td>
<td>按垂直方向（行顺序）堆叠数组构成一个新的数组，堆叠的数组需要具有相同的维度</td>
</tr>
<tr class="even">
<td>np.dstack</td>
<td>沿深度方向将数组进行堆叠。将相同尺寸的数组沿着第三个维度（深度方向）进行水平叠加，返回一个新的数组。</td>
</tr>
<tr class="odd">
<td>np.hsplit</td>
<td>将一个数组水平（列）分割成多个子数组</td>
</tr>
<tr class="even">
<td>np.vsplit</td>
<td>将一个数组垂直（行）分割成多个子数组</td>
</tr>
</tbody>
</table>
<blockquote>
<ul>
<li><p>append、concatenate以及stack都有一个axis参数，用于控制数组的合并方式是按行还是按列。</p></li>
<li><p>对于append和concatenate，待合并的数组必须有相同的行数或列数（满足一个即可）。</p></li>
<li><p>stack、hstack、dstack，要求待合并的数组必须具有相同的形状(shape)。</p></li>
<li><p>numpy.hsplit(arr, indices_or_sections)和numpy.vsplit(arr, indices_or_sections)：①arr是要拆分的多维数组或矩阵[ndarray]；②indices_or_sections : [int or 1-D array] 如果indices_or_sections是一个整数N，数组将沿轴线被分成N个相等的数组。如果indices_or_sections是一个排序的整数的一维数组，这些整数表示该数组沿轴线的分割位置。</p></li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># (1)append</span></span><br><span class="line">a = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = np.array([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">c = np.append(a, b)</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span> * <span class="number">20</span>)</span><br><span class="line">a1 = np.arange(<span class="number">4</span>).reshape(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">b1 = np.arange(<span class="number">4</span>).reshape(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 按行合并</span></span><br><span class="line">c1 = np.append(a1, b1, axis=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;按行合并后的结果&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(c1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;合并后数据维度&#x27;</span>, c1.shape)</span><br><span class="line"><span class="comment"># 按列合并</span></span><br><span class="line">d2 = np.append(a1, b1, axis=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;按列合并后的结果&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(d2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;合并后数据维度&#x27;</span>, d2.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span> * <span class="number">20</span>)</span><br><span class="line"><span class="comment"># (2)concatenate沿指定轴连接数组或矩阵</span></span><br><span class="line">a3 = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">b3 = np.array([[<span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">c3 = np.concatenate((a3, b3), axis=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(c3)</span><br><span class="line">d3 = np.concatenate((a3, b3.T), axis=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(d3)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span> * <span class="number">20</span>)</span><br><span class="line"><span class="comment"># (3)stack沿指定轴堆叠数组或矩阵,axis=0/1/2</span></span><br><span class="line">a4 = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">b4 = np.array([[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line"><span class="built_in">print</span>(np.stack((a4, b4), axis=<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span> * <span class="number">20</span>)</span><br><span class="line"><span class="comment"># (3)vstack和hstack</span></span><br><span class="line">a5 = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]])</span><br><span class="line">b5 = np.array([[<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="built_in">print</span>(np.vstack((a5, b5)))</span><br><span class="line"><span class="built_in">print</span>(np.hstack((a5, b5)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span> * <span class="number">20</span>)</span><br><span class="line"><span class="comment"># (4)dstack</span></span><br><span class="line">a6 = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">b6 = np.array([[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line"><span class="built_in">print</span>(np.dstack((a6, b6)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]</span><br><span class="line">********************</span><br><span class="line">按行合并后的结果</span><br><span class="line">[[<span class="number">0</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">3</span>]]</span><br><span class="line">合并后数据维度 (<span class="number">4</span>, <span class="number">2</span>)</span><br><span class="line">按列合并后的结果</span><br><span class="line">[[<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">3</span> <span class="number">2</span> <span class="number">3</span>]]</span><br><span class="line">合并后数据维度 (<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">********************</span><br><span class="line">[[<span class="number">1</span> <span class="number">2</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">4</span>]</span><br><span class="line"> [<span class="number">5</span> <span class="number">6</span>]]</span><br><span class="line">[[<span class="number">1</span> <span class="number">2</span> <span class="number">5</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">4</span> <span class="number">6</span>]]</span><br><span class="line">********************</span><br><span class="line">[[[<span class="number">1</span> <span class="number">2</span>]</span><br><span class="line">  [<span class="number">5</span> <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">3</span> <span class="number">4</span>]</span><br><span class="line">  [<span class="number">7</span> <span class="number">8</span>]]]</span><br><span class="line">********************</span><br><span class="line">[[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]]</span><br><span class="line">[[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]]</span><br><span class="line">********************</span><br><span class="line">[[[ <span class="number">1</span>  <span class="number">7</span>]</span><br><span class="line">  [ <span class="number">2</span>  <span class="number">8</span>]</span><br><span class="line">  [ <span class="number">3</span>  <span class="number">9</span>]]</span><br><span class="line"></span><br><span class="line"> [[ <span class="number">4</span> <span class="number">10</span>]</span><br><span class="line">  [ <span class="number">5</span> <span class="number">11</span>]</span><br><span class="line">  [ <span class="number">6</span> <span class="number">12</span>]]]</span><br></pre></td></tr></table></figure>
<h4 id="批量处理">👁批量处理</h4>
<p>在深度学习中，由于源数据都比较大，通常需要用到批处理。如利用批量来计算梯度的随机梯度法(SGD)就是一个典型应用。深度学习的计算一般比较复杂，并且数据量一般比较大，如果一次处理整个数据，较大概率会出现资源瓶颈。为了更有效地计算，一般将整个数据集分批次处理。与处理整个数据集相反的另一个极端是每次只处理一条记录，这种方法也不科学，一次处理一条记录无法充分发挥GPU、Numpy的平行处理优势。因此，在实际使用中往往采用批量处理(Mini-Batch)的方法。如何把大数据拆分成多个批次呢？可采用如下步骤：</p>
<ol type="1">
<li>数据集</li>
<li>随机打乱数据：shuffle()</li>
<li>定义批大小：batch_size</li>
<li>批处理数据集</li>
</ol>
<h4 id="通用函数">👁通用函数</h4>
<p>ufunc是universal function的缩写，它是一种能对数组的每个元素进行操作的函数。许多ufunc函数都是用C语言级别实现的，因此它们的计算速度非常快。此外，它们比math模块中的函数更灵活。math模块的输入一般是标量，但Numpy中的函数可以是向量或矩阵，而利用向量或矩阵可以避免使用循环语句，这点在机器学习、深度学习中非常重要。Numpy中的几个常用通用函数：</p>
<table>
<thead>
<tr class="header">
<th>函数</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>sqrt</td>
<td>计算序列化数据的平方根</td>
</tr>
<tr class="even">
<td>sin、cos</td>
<td>三角函数</td>
</tr>
<tr class="odd">
<td>abs</td>
<td>计算序列化数据的绝对值</td>
</tr>
<tr class="even">
<td>dot</td>
<td>矩阵运算</td>
</tr>
<tr class="odd">
<td>log、log10、1og2</td>
<td>对数运算</td>
</tr>
<tr class="even">
<td>exp</td>
<td>指数运算</td>
</tr>
<tr class="odd">
<td>cumsum、cumproduct</td>
<td>累计求和、求积</td>
</tr>
<tr class="even">
<td>sum</td>
<td>对一个序列化数据进行求和</td>
</tr>
<tr class="odd">
<td>mean</td>
<td>计算均值</td>
</tr>
<tr class="even">
<td>median</td>
<td>计算中位数</td>
</tr>
<tr class="odd">
<td>std</td>
<td>计算标准差</td>
</tr>
<tr class="even">
<td>var</td>
<td>计算方差</td>
</tr>
<tr class="odd">
<td>corrcoef</td>
<td>计算相关系数</td>
</tr>
</tbody>
</table>
<h4 id="广播机制">👁广播机制</h4>
<p>Numpy的Universal functions中要求输入的数组shape是一致的，当数组的shape不相等时，则会使用广播机制。不过，调整数组使得shape一样，需要满足一定的规则，否则将出错。这些规则可归纳为以下4条。</p>
<ul>
<li>让所有输入数组都向其中shape最长的数组看齐，不足的部分则通过在前面加1补齐；</li>
<li>输出数组的shape是输入数组shape的各个轴上的最大值；</li>
<li>如果输入数组的某个轴和输出数组的对应轴的长度相同或者某个轴的长度为1时，这个数组能被用来计算，否则出错；</li>
<li>当输入数组的某个轴的长度为1时，沿着此轴运算时都用(或复制)此轴上的第一组值。</li>
</ul>
<blockquote>
<p>例子：A+B，其中A为4×1矩阵，B为一维向量（3,）。要相加，需要做如下处理：</p>
<ol type="1">
<li>根据规则1，B需要向看齐，把B变为（1, 3）</li>
<li>根据规则2，输出的结果为各个轴上的最大值，即输出结果应该为（4, 3）矩阵，那么A如何由（4, 1）变为（4, 3）矩阵？B又如何由（1, 3）变为（4, 3）矩阵？</li>
<li>根据规则4，用此轴上的第一组值(要主要区分是哪个轴)，进行复制(但在实际处理中不是真正复制，否则太耗内存，而是采用其他对象如ogrid对象，进行网格处理)即可。</li>
</ol>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A = np.arange(<span class="number">0</span>, <span class="number">40</span>,<span class="number">10</span>).reshape(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">B = np.arange(<span class="number">0</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;A矩阵的形状:&#123;&#125;,B矩阵的形状:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(A.shape,B.shape))</span><br><span class="line">C = A+B</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;C矩阵的形状:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(C.shape))</span><br><span class="line"><span class="built_in">print</span>(C)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">A矩阵的形状:(<span class="number">4</span>, <span class="number">1</span>),B矩阵的形状:(<span class="number">3</span>,)</span><br><span class="line">C矩阵的形状:(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">[[ <span class="number">0</span>  <span class="number">1</span>  <span class="number">2</span>]</span><br><span class="line"> [<span class="number">10</span> <span class="number">11</span> <span class="number">12</span>]</span><br><span class="line"> [<span class="number">20</span> <span class="number">21</span> <span class="number">22</span>]</span><br><span class="line"> [<span class="number">30</span> <span class="number">31</span> <span class="number">32</span>]]</span><br></pre></td></tr></table></figure>
<h3 id="pytorch基础-1">👀PyTorch基础</h3>
<p>PyTorch是建立在Torch库之上的Python包，旨在加速深度学习应用。它提供一种类似Numpy的抽象方法来表征张量(或多维数组)，可以利用GPU加速训练。PyTorch采用了动态计算图(Dynamic Computational Graph)结构，且基于tape的Autograd系统的深度神经网络。很多框架，比如TensorFlow(TensorFlow2.0也加入动态网络的支持)、Caffe、CNTK、Theano等采用静态计算图。PyTorch通过一种称为反向模式自动微分的技术，可以零延迟或零成本地任意改变你的网络的行为。Torch是PyTorch中的一个重要包，它包含了多维张量的数据结构以及基于其上的多种数学操作。PyTorch由4个主要的包组成：</p>
<ul>
<li>torch：类似Numpy的通用数组库，将张量类型转换为torch.cuda.TensorFloat，并在GPU上计算。</li>
<li>torch.autograd：用于构建计算图形并自动获取梯度的包。</li>
<li>torch.nn：具有共享层和损失函数的神经网络库。</li>
<li>torch.optim：具有通用优化算法(如SGD、Adam等)的优化包。</li>
</ul>
<h4 id="tensor概述">👁Tensor概述</h4>
<p>PyTorch的Tensor，它可以是零维(又称为标量或一个数)、一维、二维及多维的数组。Tensor自称为神经网络界的Numpy，它与Numpy相似，二者可以共享内存，且之间的转换非常方便高效。它们最大的区别就是Numpy会把ndarray放在CPU中进行加速运算，而由Torch产生的Tensor会放在GPU中进行加速运算(假设当前环境有GPU)。对Tensor的操作很多，从接口的角度来划分，可以分为两类：</p>
<ul>
<li><p>torch.function，如torch.sum、torch.add等；</p></li>
<li><p>tensor.function，如tensor.view、tensor.add等。</p></li>
</ul>
<p>这些操作对大部分Tensor都是等价的，如torch.add(x,y)与x.add(y)等价。在实际使用时，可以根据个人爱好选择。如果从修改方式的角度来划分，可以分为以下两类：</p>
<ul>
<li><p>不修改自身数据，如x.add(y)，x的数据不变，返回一个新的Tensor。</p></li>
<li><p>修改自身数据，如x.add_(y)（运行符带下划线后缀），运算结果存在x中，x被修改。</p></li>
</ul>
<h4 id="创建tensor">👁创建Tensor</h4>
<p>创建Tensor的方法有很多，可以从列表或ndarray等类型进行构建，也可根据指定的形状构建。常见的创建Tensor的方法：</p>
<table>
<thead>
<tr class="header">
<th>函数</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Tensor()</td>
<td>直接从参数构造一个张量，支持List、Numpy数组</td>
</tr>
<tr class="even">
<td>eye(row, column)</td>
<td>创建指定行数、列数的二维单位Tensor</td>
</tr>
<tr class="odd">
<td>linspace(start, end, steps)</td>
<td>从start到end，均匀切分成steps份</td>
</tr>
<tr class="even">
<td>logspace(start, end, steps)</td>
<td>从10^start，到10^end，均匀切分成steps份</td>
</tr>
<tr class="odd">
<td>rand/randn(*size)</td>
<td>生成[0, 1)均匀分布/标准正态分布数据</td>
</tr>
<tr class="even">
<td>ones(*size)</td>
<td>返回指定shape的张量，元素初始为1</td>
</tr>
<tr class="odd">
<td>zeros(*size)</td>
<td>返回指定shape的张量，元素初始为0</td>
</tr>
<tr class="even">
<td>ones_like(T)</td>
<td>返回与T的shape相同的张量，且元素初始为1</td>
</tr>
<tr class="odd">
<td>zeros_like(T)</td>
<td>返回与T的shape相同的张量，且元素初始为0</td>
</tr>
<tr class="even">
<td>arange(start, end, step)</td>
<td>在区间[start, end)上以间隔step生成一个序列张量</td>
</tr>
<tr class="odd">
<td>from_Numpy(ndarray)</td>
<td>从ndarray创建一个Tensor</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据list数据生成Tensor</span></span><br><span class="line">a = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">20</span>)</span><br><span class="line"><span class="comment"># 根据指定形状生成Tensor，随机初始化</span></span><br><span class="line">b = torch.Tensor(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">20</span>)</span><br><span class="line"><span class="comment"># 根据给定的Tensor的形状</span></span><br><span class="line">c = torch.Tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="comment"># 查看Tensor的形状</span></span><br><span class="line"><span class="built_in">print</span>(c.size())</span><br><span class="line"><span class="comment"># shape与size()等价方式</span></span><br><span class="line"><span class="built_in">print</span>(c.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">20</span>)</span><br><span class="line"><span class="comment"># 根据已有形状创建Tensor，随机初始化</span></span><br><span class="line">d = torch.Tensor(c.size())</span><br><span class="line"><span class="built_in">print</span>(d)</span><br><span class="line"><span class="comment"># 生成一个单位矩阵</span></span><br><span class="line">torch.eye(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 自动生成全是0的矩阵</span></span><br><span class="line">torch.zeros(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 根据规则生成数据</span></span><br><span class="line">torch.linspace(<span class="number">1</span>, <span class="number">10</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment"># 生成满足均匀分布随机数</span></span><br><span class="line">torch.rand(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 生成满足标准分布随机数</span></span><br><span class="line">torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 返回所给数据形状相同，值全为0的张量</span></span><br><span class="line">torch.zeros_like(torch.rand(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>])</span><br><span class="line">********************</span><br><span class="line">tensor([[<span class="number">7.2564e+24</span>, <span class="number">1.7418e-42</span>, <span class="number">0.0000e+00</span>],</span><br><span class="line">        [<span class="number">0.0000e+00</span>, <span class="number">0.0000e+00</span>, <span class="number">0.0000e+00</span>]])</span><br><span class="line">********************</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>]])</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">********************</span><br><span class="line">tensor([[<span class="number">7.2567e+24</span>, <span class="number">1.7418e-42</span>, <span class="number">0.0000e+00</span>],</span><br><span class="line">        [<span class="number">0.0000e+00</span>, <span class="number">0.0000e+00</span>, <span class="number">0.0000e+00</span>]])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意torch.Tensor与torch.tensor的几点区别：</p>
<p>1）torch.Tensor是torch.empty和torch.tensor之间的一种混合，但当传入数据时，torch.Tensor使用全局默认dtype(FloatTensor)，而torch.tensor是从数据中推断数据类型。</p>
<p>2）torch.tensor(1)返回一个固定值1，而torch.Tensor(1)返回一个大小为1的张量，它是<strong>随机初始化</strong>的值。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">t1 = torch.Tensor(<span class="number">3</span>)</span><br><span class="line">t2 = torch.Tensor(<span class="number">1</span>)</span><br><span class="line">t3 = torch.tensor(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;t1的值&#123;&#125;,t1的数据类型&#123;&#125;&quot;</span>.<span class="built_in">format</span>(t1, t1.<span class="built_in">type</span>()))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;t2的值&#123;&#125;,t2的数据类型&#123;&#125;&quot;</span>.<span class="built_in">format</span>(t2, t2.<span class="built_in">type</span>()))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;t3的值&#123;&#125;,t3的数据类型&#123;&#125;&quot;</span>.<span class="built_in">format</span>(t3, t3.<span class="built_in">type</span>()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果1</span></span><br><span class="line">t1的值tensor([<span class="number">7.0406e+13</span>, <span class="number">1.1561e-42</span>, <span class="number">0.0000e+00</span>]),t1的数据类型torch.FloatTensor</span><br><span class="line">t2的值tensor([<span class="number">0.</span>]),t2的数据类型torch.FloatTensor</span><br><span class="line">t3的值<span class="number">1</span>,t3的数据类型torch.LongTensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果2</span></span><br><span class="line">t1的值tensor([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]),t1的数据类型torch.FloatTensor</span><br><span class="line">t2的值tensor([<span class="number">3.4713e-18</span>]),t2的数据类型torch.FloatTensor</span><br><span class="line">t3的值<span class="number">1</span>,t3的数据类型torch.LongTensor</span><br></pre></td></tr></table></figure>
<h4 id="修改tensor形状">👁修改Tensor形状</h4>
<p>在处理数据、构建网络层等过程中，需要了解Tensor的形状、修改Tensor的形状。与修改Numpy的形状类似，修改Tenor的形状也有很多类似函数：</p>
<table>
<colgroup>
<col style="width: 16%" />
<col style="width: 83%" />
</colgroup>
<thead>
<tr class="header">
<th>函数</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>size()</td>
<td>返回张量的shape属性值，与属性shape(0.4版新增)等价</td>
</tr>
<tr class="even">
<td>numel(input)</td>
<td>计算Tensor的元素个数</td>
</tr>
<tr class="odd">
<td>view(*shape)</td>
<td>修改Tensor的shape，与reshape(0.4版新增)类似，但view返回的对象与源Tensor共享内存，修改一个，另一个同时修改。reshape将生成新的Tensor，而且不要求源Tensor是连续的。view(-1)展平数组</td>
</tr>
<tr class="even">
<td>resize</td>
<td>类似于view，但在size超出时会重新分配内存空间</td>
</tr>
<tr class="odd">
<td>item</td>
<td>若Tensor为<strong>单(一个)元素</strong>，则返回Python的标量</td>
</tr>
<tr class="even">
<td>unsqueeze</td>
<td>扩展维度，在插入维度的索引位置增加维度"1",这里的"1"仅仅起到扩展维度的作用</td>
</tr>
<tr class="odd">
<td>squeeze</td>
<td>降低维度，在压缩维度的索引位置减少维度"1"，即将输入张量形状指定位置中的"1"去除并返回</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成一个形状为2x3的矩阵</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 查看矩阵的形状</span></span><br><span class="line"><span class="built_in">print</span>(x.size())</span><br><span class="line"><span class="comment"># 查看x的维度</span></span><br><span class="line"><span class="built_in">print</span>(x.dim())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">20</span>)</span><br><span class="line"><span class="comment"># 把x变为3x2的矩阵</span></span><br><span class="line"><span class="built_in">print</span>(x.view(<span class="number">3</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># 把x展平为1维向量</span></span><br><span class="line">y = x.view(-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">20</span>)</span><br><span class="line"><span class="comment"># 添加一个维度</span></span><br><span class="line">z = torch.unsqueeze(y, <span class="number">0</span>)</span><br><span class="line"><span class="comment"># 查看z的形状</span></span><br><span class="line"><span class="built_in">print</span>(z.size())</span><br><span class="line"><span class="comment"># 计算Z的元素个数</span></span><br><span class="line"><span class="built_in">print</span>(z.numel())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">20</span>)</span><br><span class="line"><span class="comment"># item()的用法</span></span><br><span class="line">a = torch.tensor([<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(a.item())</span><br><span class="line">b = torch.tensor([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"><span class="built_in">print</span>(b.tolist())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="number">2</span></span><br><span class="line">********************</span><br><span class="line">tensor([[-<span class="number">0.4888</span>,  <span class="number">1.4943</span>],</span><br><span class="line">        [-<span class="number">1.6593</span>, -<span class="number">0.6757</span>],</span><br><span class="line">        [-<span class="number">1.9228</span>, -<span class="number">1.0863</span>]])</span><br><span class="line">torch.Size([<span class="number">6</span>])</span><br><span class="line">********************</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">6</span>])</span><br><span class="line"><span class="number">6</span></span><br><span class="line">********************</span><br><span class="line"><span class="number">1</span></span><br><span class="line">[<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>torch.view与torch.reshpae的异同</p>
<p>1）reshape()可由torch.reshape()，也可由torch.Tensor.reshape()调用。但view()只可由torch.Tensor.view()来调用。</p>
<p>2）对于一个将要被view的Tensor，新的size必须与原来的size与stride兼容。否则，在view之前必须调用contiguous()方法。</p>
<p>3）返回与input数据量相同，但形状不同的Tensor。若满足view的条件，则不会copy，若不满足，则会copy。</p>
<p>4）如果只是重塑张量，请使用torch.reshape。如果还关注内存使用情况并希望确保两个张量共享相同的数据，请使用torch.view。</p>
</blockquote>
<h4 id="索引操作">👁索引操作</h4>
<p>Tensor的索引操作与Numpy类似，一般情况下索引结果与源数据共享内存。从Tensor获取元素除了可以通过索引，也可以借助一些函数，常用的选择函数如下：</p>
<table>
<thead>
<tr class="header">
<th>函数</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>index_select(input,, dim, index)</td>
<td>在指定维度上选择行和列</td>
</tr>
<tr class="even">
<td>nonzero(input)</td>
<td>获取非0元素的下标</td>
</tr>
<tr class="odd">
<td>masked_select(input, mask)</td>
<td>使用二元值进行选择</td>
</tr>
<tr class="even">
<td>gather(input, dim, index)</td>
<td>在指定维度上选择数据，输出形状与index(类型是LongTensor)一致</td>
</tr>
<tr class="odd">
<td>scatter__(dim, index, src)</td>
<td>为gather的反操作，根据指定索引补充数据</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置一个随机种子</span></span><br><span class="line">torch.manual_seed(<span class="number">100</span>)</span><br><span class="line"><span class="comment"># 生成一个形状为2x3的矩阵</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="comment"># 根据索引获取第1行，所有数据</span></span><br><span class="line"><span class="built_in">print</span>(x[<span class="number">0</span>, :])</span><br><span class="line"><span class="comment"># 获取最后一列数据</span></span><br><span class="line"><span class="built_in">print</span>(x[:, -<span class="number">1</span>])</span><br><span class="line"><span class="comment"># 生成是否大于0的Byter张量</span></span><br><span class="line">mask = x &gt; <span class="number">0</span></span><br><span class="line"><span class="built_in">print</span>(mask)</span><br><span class="line"><span class="comment"># 获取大于0的值</span></span><br><span class="line"><span class="built_in">print</span>(torch.masked_select(x, mask))</span><br><span class="line"><span class="comment"># 获取非0下标,即行，列索引</span></span><br><span class="line"><span class="built_in">print</span>(torch.nonzero(mask))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">20</span>)</span><br><span class="line"><span class="comment"># 获取指定索引对应的值</span></span><br><span class="line">index1 = torch.LongTensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line"><span class="built_in">print</span>(torch.gather(x, <span class="number">0</span>, index1))</span><br><span class="line">index2 = torch.LongTensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">a = torch.gather(x, <span class="number">1</span>, index2)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># 把a的值返回到一个2x3的0矩阵中</span></span><br><span class="line">z = torch.zeros(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">z.scatter_(<span class="number">1</span>, index2, a)</span><br><span class="line"><span class="built_in">print</span>(z)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">tensor([[ <span class="number">0.3607</span>, -<span class="number">0.2859</span>, -<span class="number">0.3938</span>],</span><br><span class="line">        [ <span class="number">0.2429</span>, -<span class="number">1.3833</span>, -<span class="number">2.3134</span>]])</span><br><span class="line">tensor([ <span class="number">0.3607</span>, -<span class="number">0.2859</span>, -<span class="number">0.3938</span>])</span><br><span class="line">tensor([-<span class="number">0.3938</span>, -<span class="number">2.3134</span>])</span><br><span class="line">tensor([[ <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>],</span><br><span class="line">        [ <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>]])</span><br><span class="line">tensor([<span class="number">0.3607</span>, <span class="number">0.2429</span>])</span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line">********************</span><br><span class="line">tensor([[ <span class="number">0.3607</span>, -<span class="number">1.3833</span>, -<span class="number">2.3134</span>]])</span><br><span class="line">tensor([[ <span class="number">0.3607</span>, -<span class="number">0.2859</span>, -<span class="number">0.2859</span>],</span><br><span class="line">        [-<span class="number">1.3833</span>, -<span class="number">1.3833</span>, -<span class="number">1.3833</span>]])</span><br><span class="line">tensor([[ <span class="number">0.3607</span>, -<span class="number">0.2859</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [ <span class="number">0.0000</span>, -<span class="number">1.3833</span>,  <span class="number">0.0000</span>]])</span><br></pre></td></tr></table></figure>
<h4 id="广播机制-1">👁广播机制</h4>
<p>PyTorch也支持广播机制：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">A = np.arange(<span class="number">0</span>, <span class="number">40</span>, <span class="number">10</span>).reshape(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">B = np.arange(<span class="number">0</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 把ndarray转换为Tensor</span></span><br><span class="line">A1 = torch.from_numpy(A)  <span class="comment"># 形状为4x1</span></span><br><span class="line">B1 = torch.from_numpy(B)  <span class="comment"># 形状为3</span></span><br><span class="line"><span class="comment"># Tensor自动实现广播</span></span><br><span class="line">C = A1 + B1</span><br><span class="line"><span class="built_in">print</span>(C)</span><br><span class="line"><span class="comment"># 我们可以根据广播机制，手工进行配置</span></span><br><span class="line"><span class="comment"># 根据规则1，B1需要向A1看齐，把B变为（1,3）</span></span><br><span class="line">B2 = B1.unsqueeze(<span class="number">0</span>)  <span class="comment"># B2的形状为1x3</span></span><br><span class="line"><span class="comment"># 使用expand函数重复数组，分别的4x3的矩阵</span></span><br><span class="line">A2 = A1.expand(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">B3 = B2.expand(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 然后进行相加,C1与C结果一致</span></span><br><span class="line">C1 = A2 + B3</span><br><span class="line"><span class="built_in">print</span>(C1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">tensor([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>],</span><br><span class="line">        [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>],</span><br><span class="line">        [<span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>],</span><br><span class="line">        [<span class="number">30</span>, <span class="number">31</span>, <span class="number">32</span>]], dtype=torch.int32)</span><br><span class="line">tensor([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>],</span><br><span class="line">        [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>],</span><br><span class="line">        [<span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>],</span><br><span class="line">        [<span class="number">30</span>, <span class="number">31</span>, <span class="number">32</span>]], dtype=torch.int32)</span><br></pre></td></tr></table></figure>
<h4 id="逐元素操作">👁逐元素操作</h4>
<p>与Numpy一样，Tensor也有逐元素操作(Element-Wise)，且操作内容相似，但使用函数可能不尽相同。大部分数学运算都属于逐元素操作，其输入与输出的形状相同。这些操作均会创建新的Tensor，如果需要就地操作，可以使用这些方法的下划线版本，例如abs_。常见的函数如下：</p>
<table>
<thead>
<tr class="header">
<th>函数</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>abs/add</td>
<td>绝对值/加法</td>
</tr>
<tr class="even">
<td>addcdiv(t, v, t1, t2)</td>
<td>t1与t2的按元素除后，乘v加t；（可能弹出"UserWarning"）</td>
</tr>
<tr class="odd">
<td>addcmul(t, v, t1, t2)</td>
<td>t1与t2的按元素乘后，乘v加t；（可能弹出"UserWarning"）</td>
</tr>
<tr class="even">
<td>ceil/floor</td>
<td>向上取整/向下取整</td>
</tr>
<tr class="odd">
<td>clamp(t, min, max)</td>
<td>将张量元素限制在指定区间</td>
</tr>
<tr class="even">
<td>exp/log/pow</td>
<td>指数/对数/幂</td>
</tr>
<tr class="odd">
<td>mul(或*)/neg</td>
<td>逐元素乘法/取反</td>
</tr>
<tr class="even">
<td>sigmoid/tanh/softmax</td>
<td>激活函数</td>
</tr>
<tr class="odd">
<td>sign/sqrt</td>
<td>取输入张量元素的符号/开根号</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">t = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">t1 = torch.randn(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">t2 = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(t)</span><br><span class="line"><span class="built_in">print</span>(t1)</span><br><span class="line"><span class="built_in">print</span>(t2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">20</span>)</span><br><span class="line"><span class="built_in">print</span>(torch.addcdiv(t, <span class="number">0.1</span>, t1, t2))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">20</span>)</span><br><span class="line"><span class="comment"># 计算sigmoid</span></span><br><span class="line"><span class="built_in">print</span>(torch.sigmoid(t))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">20</span>)</span><br><span class="line"><span class="comment"># 将t限制在[0,1]之间</span></span><br><span class="line"><span class="built_in">print</span>(torch.clamp(t, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">20</span>)</span><br><span class="line"><span class="comment"># t+2进行就地运算</span></span><br><span class="line">t.add_(<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(t)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">tensor([[ <span class="number">2.4080</span>,  <span class="number">0.6269</span>, -<span class="number">0.0157</span>]])</span><br><span class="line">tensor([[-<span class="number">0.3307</span>],</span><br><span class="line">        [-<span class="number">0.6533</span>],</span><br><span class="line">        [ <span class="number">0.7257</span>]])</span><br><span class="line">tensor([[ <span class="number">0.6907</span>, -<span class="number">0.8531</span>, -<span class="number">0.8397</span>]])</span><br><span class="line">********************</span><br><span class="line">tensor([[ <span class="number">2.3601</span>,  <span class="number">0.6656</span>,  <span class="number">0.0237</span>],</span><br><span class="line">        [ <span class="number">2.3134</span>,  <span class="number">0.7035</span>,  <span class="number">0.0621</span>],</span><br><span class="line">        [ <span class="number">2.5131</span>,  <span class="number">0.5418</span>, -<span class="number">0.1021</span>]])</span><br><span class="line">********************</span><br><span class="line">tensor([[<span class="number">0.9174</span>, <span class="number">0.6518</span>, <span class="number">0.4961</span>]])</span><br><span class="line">********************</span><br><span class="line">tensor([[<span class="number">1.0000</span>, <span class="number">0.6269</span>, <span class="number">0.0000</span>]])</span><br><span class="line">********************</span><br><span class="line">tensor([[<span class="number">4.4080</span>, <span class="number">2.6269</span>, <span class="number">1.9843</span>]])</span><br></pre></td></tr></table></figure>
<h4 id="归并操作">👁归并操作</h4>
<p>归并操作，就是对输入进行归并或合计等操作，这类操作的输入输出形状一般并不相同，而且往往是输入大于输出形状。归并操作可以对整个Tensor，也可以沿着某个维度进行归并。常见的函数如下：</p>
<table>
<thead>
<tr class="header">
<th>函数</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>cumprod(t, axis)</td>
<td>在指定维度对t进行累积</td>
</tr>
<tr class="even">
<td>cumsum</td>
<td>在指定维度对t进行累加</td>
</tr>
<tr class="odd">
<td>dist(a, b, p=2)</td>
<td>返回a, b之间的p阶范数</td>
</tr>
<tr class="even">
<td>mean/median</td>
<td>均值/中位数</td>
</tr>
<tr class="odd">
<td>std/var</td>
<td>标准差/方差</td>
</tr>
<tr class="even">
<td>norm(t, p=2)</td>
<td>返回t的p阶范数</td>
</tr>
<tr class="odd">
<td>prod(t)/sum(t)</td>
<td>返回t所有元素的积/和</td>
</tr>
</tbody>
</table>
<blockquote>
<p>归并操作一般涉及一个dim参数，指定沿哪个维进行归并。另一个参数是keepdim，说明输出结果中是否保留维度1，缺省情况是False，即不保留。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成一个含6个数的向量</span></span><br><span class="line">a = torch.linspace(<span class="number">0</span>, <span class="number">10</span>, <span class="number">6</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">20</span>)</span><br><span class="line"><span class="comment"># 使用view方法，把a变为2x3矩阵</span></span><br><span class="line">a = a.view((<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">20</span>)</span><br><span class="line"><span class="comment"># 沿y轴方向累加，即dim=0</span></span><br><span class="line">b = a.<span class="built_in">sum</span>(dim=<span class="number">0</span>)  <span class="comment"># b的形状为[3]</span></span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="built_in">print</span>(b.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">20</span>)</span><br><span class="line"><span class="comment"># 沿y轴方向累加，即dim=0,并保留含1的维度</span></span><br><span class="line">b = a.<span class="built_in">sum</span>(dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>)  <span class="comment"># b的形状为[1,3]</span></span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="built_in">print</span>(b.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">tensor([ <span class="number">0.</span>,  <span class="number">2.</span>,  <span class="number">4.</span>,  <span class="number">6.</span>,  <span class="number">8.</span>, <span class="number">10.</span>])</span><br><span class="line">********************</span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">2.</span>,  <span class="number">4.</span>],</span><br><span class="line">        [ <span class="number">6.</span>,  <span class="number">8.</span>, <span class="number">10.</span>]])</span><br><span class="line">********************</span><br><span class="line">tensor([ <span class="number">6.</span>, <span class="number">10.</span>, <span class="number">14.</span>])</span><br><span class="line">torch.Size([<span class="number">3</span>])</span><br><span class="line">********************</span><br><span class="line">tensor([[ <span class="number">6.</span>, <span class="number">10.</span>, <span class="number">14.</span>]])</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<h4 id="比较操作">👁比较操作</h4>
<p>比较操作一般是进行逐元素比较，有些是按指定方向比较。常见的函数如下：</p>
<table>
<thead>
<tr class="header">
<th>函数</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>eq</td>
<td>比较Tensor是否相等(<strong>逐元素比较</strong>)，支持broadcast</td>
</tr>
<tr class="even">
<td>equal</td>
<td>比较Tensor是否有相同的shape与值(<strong>张量比较</strong>)</td>
</tr>
<tr class="odd">
<td>ge/le/gt/lt</td>
<td>大于等于/小于等于/大于/小于</td>
</tr>
<tr class="even">
<td>max/min(t, axis)</td>
<td>返回最值，若指定axis，则额外返回下标</td>
</tr>
<tr class="odd">
<td>topk(t, k, axis)</td>
<td>在指定的axis维上取最高的K个值</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.linspace(<span class="number">0</span>, <span class="number">10</span>, <span class="number">6</span>).view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">20</span>)</span><br><span class="line"><span class="comment"># 求所有元素的最大值</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">max</span>(x))  <span class="comment"># 结果为10</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">20</span>)</span><br><span class="line"><span class="comment"># 求y轴方向的最大值</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">max</span>(x, dim=<span class="number">0</span>))  <span class="comment"># 结果为[6, 8, 10]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">20</span>)</span><br><span class="line"><span class="comment"># 求最大的2个元素</span></span><br><span class="line"><span class="comment"># 结果为[6, 8, 10],对应索引为tensor([[1, 1, 1]</span></span><br><span class="line"><span class="built_in">print</span>(torch.topk(x, <span class="number">1</span>, dim=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">2.</span>,  <span class="number">4.</span>],</span><br><span class="line">        [ <span class="number">6.</span>,  <span class="number">8.</span>, <span class="number">10.</span>]])</span><br><span class="line">********************</span><br><span class="line">tensor(<span class="number">10.</span>)</span><br><span class="line">********************</span><br><span class="line">torch.return_types.<span class="built_in">max</span>(</span><br><span class="line">values=tensor([ <span class="number">6.</span>,  <span class="number">8.</span>, <span class="number">10.</span>]),</span><br><span class="line">indices=tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">********************</span><br><span class="line">torch.return_types.topk(</span><br><span class="line">values=tensor([[ <span class="number">6.</span>,  <span class="number">8.</span>, <span class="number">10.</span>]]),</span><br><span class="line">indices=tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]]))</span><br></pre></td></tr></table></figure>
<h4 id="矩阵操作">👁矩阵操作</h4>
<p>机器学习和深度学习中存在大量的矩阵运算，常用的算法有两种：一种是逐元素乘法，另外一种是点积乘法。常见的函数如下：</p>
<table>
<thead>
<tr class="header">
<th>函数</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dot(t1, t2)</td>
<td>计算张量（1D）的内积或点积</td>
</tr>
<tr class="even">
<td>mm(mat1, mat2)/bmm(batch1, batch2)</td>
<td>计算矩阵乘法/含batch的3D矩阵乘法</td>
</tr>
<tr class="odd">
<td>mv(t1, v1)</td>
<td>计算矩阵与向量乘法</td>
</tr>
<tr class="even">
<td>t</td>
<td>转置</td>
</tr>
<tr class="odd">
<td>svd(t)</td>
<td>计算t的SVD分解</td>
</tr>
</tbody>
</table>
<blockquote>
<ul>
<li><p>Torch的dot与Numpy的dot有点不同，Torch中的dot是对两个为1D张量进行点积运算，Numpy中的dot无此限制。</p></li>
<li><p>mm是对2D的矩阵进行点积，bmm对含batch的3D进行点积运算。</p></li>
<li><p>转置运算会导致存储空间不连续，需要调用contiguous方法转为连续。</p></li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.tensor([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = torch.tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(torch.dot(a, b))    <span class="comment"># 运行结果为18</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">20</span>)</span><br><span class="line">x = torch.randint(<span class="number">10</span>, (<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">y = torch.randint(<span class="number">6</span>, (<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line"><span class="built_in">print</span>(torch.mm(x, y))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">20</span>)</span><br><span class="line">x = torch.randint(<span class="number">10</span>, (<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">y = torch.randint(<span class="number">6</span>, (<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line"><span class="built_in">print</span>(torch.bmm(x, y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">tensor(<span class="number">18</span>)</span><br><span class="line">********************</span><br><span class="line">tensor([[<span class="number">47</span>, <span class="number">20</span>, <span class="number">20</span>, <span class="number">39</span>],</span><br><span class="line">        [<span class="number">55</span>, <span class="number">22</span>, <span class="number">35</span>, <span class="number">27</span>]])</span><br><span class="line">********************</span><br><span class="line">tensor([[[ <span class="number">57</span>,  <span class="number">45</span>,  <span class="number">40</span>,  <span class="number">60</span>],</span><br><span class="line">         [ <span class="number">47</span>,  <span class="number">10</span>,  <span class="number">24</span>,  <span class="number">43</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">54</span>, <span class="number">106</span>,  <span class="number">36</span>, <span class="number">104</span>],</span><br><span class="line">         [ <span class="number">34</span>,  <span class="number">42</span>,   <span class="number">4</span>,  <span class="number">50</span>]]])</span><br></pre></td></tr></table></figure>
<h4 id="pytorch与numpy比较">👁PyTorch与Numpy比较</h4>
<p>PyTorch与Numpy有很多类似的地方，并且有很多相同的操作函数名称，或虽然函数名称不同但含义相同；当然也有一些虽然函数名称相同，但含义不尽相同。</p>
<table>
<colgroup>
<col style="width: 12%" />
<col style="width: 40%" />
<col style="width: 47%" />
</colgroup>
<thead>
<tr class="header">
<th>操作类别</th>
<th>Numpy</th>
<th>PyTorch</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>数据类型</td>
<td>np.ndarray</td>
<td>torch.Tensor</td>
</tr>
<tr class="even">
<td></td>
<td>np.float32</td>
<td>torch.float32; torch.float</td>
</tr>
<tr class="odd">
<td></td>
<td>np.float64</td>
<td>torch.float64; torch.double</td>
</tr>
<tr class="even">
<td></td>
<td>np.int64</td>
<td>torch.int64; torch.long</td>
</tr>
<tr class="odd">
<td>已有数据构建</td>
<td>np.array([1.3, 1.4], dtype=np.float32)</td>
<td>torch.tensor([1.3, 1.4], dtype=torch.float32)</td>
</tr>
<tr class="even">
<td></td>
<td>x.copy()</td>
<td>x.clone()</td>
</tr>
<tr class="odd">
<td></td>
<td>np.concatenate</td>
<td>torch.cat</td>
</tr>
<tr class="even">
<td>线性代数</td>
<td>np.dot</td>
<td>torch.mm</td>
</tr>
<tr class="odd">
<td>属性</td>
<td>x.ndim</td>
<td>x.dim()</td>
</tr>
<tr class="even">
<td></td>
<td>x.size</td>
<td>x.nelement()</td>
</tr>
<tr class="odd">
<td>形状操作</td>
<td>x.reshape</td>
<td>x.reshape; x.view</td>
</tr>
<tr class="even">
<td></td>
<td>x.flatten</td>
<td>x.view(-1)</td>
</tr>
<tr class="odd">
<td>类型转换</td>
<td>np.floor(x)</td>
<td>torch.floor(x); x.floor()</td>
</tr>
<tr class="even">
<td>比较</td>
<td>np.less</td>
<td>x.lt</td>
</tr>
<tr class="odd">
<td></td>
<td>np.less_equal/np.greater</td>
<td>x.le/x.gt</td>
</tr>
<tr class="even">
<td></td>
<td>np.greater_equal/np.equal/np.not_equal</td>
<td>x.ge/x.eq/x.ne</td>
</tr>
<tr class="odd">
<td>随机种子</td>
<td>np.random.seed</td>
<td>torch.manual.seed</td>
</tr>
</tbody>
</table>
<h3 id="tensor与autograd">👀Tensor与Autograd</h3>
<blockquote>
<p>神经网络一个重要内容就是进行参数学习，而参数学习离不开求导，那么PyTorch是如何进行求导的呢？</p>
</blockquote>
<p>现在大部分深度学习架构都有自动求导的功能，PyTorch也不例外，torch.autograd包就是用来自动求导的。Autograd包为张量上所有的操作提供了自动求导功能，而torch.Tensor和torch.Function为Autograd的两个核心类，它们相互连接并生成一个有向非循环图。为实现对Tensor自动求导，需考虑如下事项：</p>
<ul>
<li><p>创建叶子节点(Leaf Node)的Tensor，使用requires_grad参数指定是否记录对其的操作，以便之后利用backward()方法进行梯度求解。requires_grad参数的缺省值为False，如果要对其求导需设置为True，然后与之有依赖关系的节点会自动变为True。</p></li>
<li><p>可利用requires_grad_()方法修改Tensor的requires_grad属性。可以调用.detach()或with torch.no_grad()，将不再计算张量的梯度，跟踪张量的历史记录。这点在评估模型、测试模型阶段中常常用到。</p></li>
<li><p>通过运算创建的Tensor（即非叶子节点），会自动被赋予grad_fn属性，该属性表示梯度函数。叶子节点的grad_fn为None。</p></li>
<li><p>最后得到的Tensor执行backward()函数，此时自动计算各变量的梯度，并将累加结果保存到grad属性中。计算完成后，非叶子节点的梯度自动释放。</p></li>
<li><p>backward()函数接收参数，该参数应和调用backward()函数的Tensor的维度相同，或者是可broadcast的维度。如果求导的Tensor为标量（即一个数字），则backward中的参数可省略。</p></li>
<li><p>反向传播的中间缓存会被清空，如果需进行多次反向传播，需指定backward中的参数retain_graph=True。多次反向传播时，梯度是累加的。</p></li>
<li><p>非叶子节点的梯度backward调用后即被清空。</p></li>
<li><p>可以通过用torch.no_grad()包裹代码块的形式来阻止autograd去跟踪那些标记为.requesgrad=True的张量的历史记录。这步在测试阶段经常使用。</p></li>
</ul>
<blockquote>
<p><strong>标量反向传播</strong>：当目标张量为标量时，可以调用backward()方法且无须传入参数。目标张量一般都是标量，如我们经常使用的损失值Loss，一般都是一个标量。</p>
<p><strong>非标量反向传播</strong>：PyTorch规定，不让张量(Tensor)对张量求导，只允许标量对张量求导。因此，如果目标张量对一个非标量调用backward()，则需要传入gradient参数，该参数也是张量，而且需要与调用backward()的张量形状相同。传入这个参数就是为了把张量对张量的求导转换为标量对张量的求导。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 标量反向传播</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入张量x</span></span><br><span class="line">x = torch.Tensor([<span class="number">2</span>])</span><br><span class="line"><span class="comment"># 初始化权重参数W,偏移量b、并设置require_grad属性为True，为自动求导</span></span><br><span class="line">w = torch.randn(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.randn(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 实现前向传播</span></span><br><span class="line">y = torch.mul(w, x)  <span class="comment"># 等价于w*x</span></span><br><span class="line">z = torch.add(y, b)  <span class="comment"># 等价于y+b</span></span><br><span class="line"><span class="comment"># 查看x,w，b页子节点的requite_grad属性</span></span><br><span class="line"><span class="comment"># x,w,b的require_grad属性分别为：False,True,True</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x, w, b的require_grad属性分别为：&#123;&#125;,&#123;&#125;,&#123;&#125;&quot;</span>.<span class="built_in">format</span>(x.requires_grad, w.requires_grad, b.requires_grad))</span><br><span class="line"><span class="comment"># 查看非叶子节点的requres_grad属性</span></span><br><span class="line"><span class="comment"># 因与w，b有依赖关系，故y，z的requires_grad属性也是：True,True</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y, z的requires_grad属性分别为：&#123;&#125;,&#123;&#125;&quot;</span>.<span class="built_in">format</span>(y.requires_grad, z.requires_grad))</span><br><span class="line"><span class="comment"># 查看各节点是否为叶子节点</span></span><br><span class="line"><span class="comment"># x，w，b，y，z的是否为叶子节点：True,True,True,False,False</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x, w, b, y, z的是否为叶子节点：&#123;&#125;,&#123;&#125;,&#123;&#125;,&#123;&#125;,&#123;&#125;&quot;</span>.<span class="built_in">format</span>(x.is_leaf, w.is_leaf, b.is_leaf, y.is_leaf, z.is_leaf))</span><br><span class="line"><span class="comment"># 查看叶子节点的grad_fn属性</span></span><br><span class="line"><span class="comment"># 因x，w，b为用户创建的，为通过其他张量计算得到，故x，w，b的grad_fn属性：None,None,None</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x, w, b的grad_fn属性：&#123;&#125;,&#123;&#125;,&#123;&#125;&quot;</span>.<span class="built_in">format</span>(x.grad_fn, w.grad_fn, b.grad_fn))</span><br><span class="line"><span class="comment"># 查看非叶子节点的grad_fn属性</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y, z的是否为叶子节点：&#123;&#125;,&#123;&#125;&quot;</span>.<span class="built_in">format</span>(y.grad_fn, z.grad_fn))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">x, w, b的require_grad属性分别为：<span class="literal">False</span>,<span class="literal">True</span>,<span class="literal">True</span></span><br><span class="line">y, z的requires_grad属性分别为：<span class="literal">True</span>,<span class="literal">True</span></span><br><span class="line">x, w, b, y, z的是否为叶子节点：<span class="literal">True</span>,<span class="literal">True</span>,<span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span></span><br><span class="line">x, w, b的grad_fn属性：<span class="literal">None</span>,<span class="literal">None</span>,<span class="literal">None</span></span><br><span class="line">y, z的是否为叶子节点：&lt;MulBackward0 <span class="built_in">object</span> at <span class="number">0x000001DCB56E94C0</span>&gt;,&lt;AddBackward0 <span class="built_in">object</span> at <span class="number">0x000001DCB56E92B0</span>&gt;</span><br></pre></td></tr></table></figure>
<h2 id="pytorch神经网络工具箱">⛄PyTorch神经网络工具箱</h2>
<p>PyTorch的神经网络工具箱，可以极大简化我们构建模型的任务，设计一个神经网络就像搭积木一样。神经网络核心组件主要包括：</p>
<ul>
<li><p>层：神经网络的基本结构，将输入张量转换为输出张量。</p></li>
<li><p>模型：层构成的网络。</p></li>
<li><p>损失函数：参数学习的目标函数，通过最小化损失函数来学习各种参数。</p></li>
<li><p>优化器：如何使损失函数最小，这就涉及优化器。</p></li>
</ul>
<p>多个层链接在一起构成一个模型或网络，输入数据通过这个模型转换为预测值，然后损失函数把预测值与真实值进行比较，得到损失值（损失值可以是距离、概率值等），该损失值用于衡量预测值与目标结果的匹配或相似程度，优化器利用损失值更新权重参数，从而使损失值越来越小。这是一个循环过程，当损失值达到一个阀值或循环次数到达指定次数，循环结束。PyTorch构建神经网络使用的主要工具（或类）及相互关系如下图所示：</p>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pkJs2pq"><img src="/img/loading1.gif" data-original="https://s21.ax1x.com/2024/06/04/pkJs2pq.jpg" alt="pkJs2pq.jpg" /></a></p>
<p>构建网络层可以基于Module类或函数(nn.functional)。nn中的大多数层(Layer)在functional中都有与之对应的函数。nn.functional中函数与nn.Module中的Layer的主要区别是后者继承Module类，会自动提取可学习的参数。而nn.functional更像是纯函数。两者功能相同，且性能也没有很大区别，那么如何选择呢？像卷积层、全连接层、Dropout层等因含有可学习参数，一般使用nn.Module，而激活函数、池化层不含可学习参数，可以使用nn.functional中对应的函数。</p>
<blockquote>
<ul>
<li><strong>构建网络</strong>：使用sequential构建网络，Sequential()函数的功能是将网络的层组合到一起。</li>
<li><strong>前向传播</strong>：①forward()函数的任务是把输入层、网络层、输出层链接起来，实现信息的前向传导，该函数的参数一般为输入数据，返回值为输出数据。②在forward函数中，有些层来自nn.Module，也可以使用nn.functional定义。来自nn.Module的需要实例化，而使用nn.functional定义的可以直接使用。</li>
<li><strong>反向传播</strong>：①手工实现反向传播，比较费时；②PyTorch提供了自动反向传播的功能，直接让损失函数调用backward()即可。</li>
<li><strong>训练模型</strong>：①训练模型时需要使模型处于训练模式，即调用model.train()，会把所有的module设置为训练模式。如果是测试或验证阶段，需要使模型处于验证阶段，即调用model.eval()，会把所有的training属性设置为False。②缺省情况下梯度是累加的，需要调用optimizer.zero_grad()把梯度初始化或清零。③训练过程中，正向传播生成网络的输出，计算输出和实际值之间的损失值，调用loss.backward()自动生成梯度，然后使用optimizer.step()执行优化器，把梯度传播回每个网络。④如果希望用GPU训练，需要把模型、训练数据、测试数据发送到GPU上，即调用.to(device)。如果需要使用多GPU进行处理，可使模型或相关数据引用nn.DataParallel。</li>
</ul>
</blockquote>
<h3 id="nn.module">👀nn.Module</h3>
<p>nn.Module是nn的一个核心数据结构，它可以是神经网络的某个层(Layer)，也可以是包含多层的神经网络。在实际使用中，最常见的做法是继承nn.Module，生成自己的网络/层。nn中已实现了绝大多数层，包括全连接层、损失层、激活层、卷积层、循环层等，这些层都是nn.Module的子类，能够自动检测到自己的Parameter，并将其作为学习参数，且针对GPU运行进行了cuDNN优化。</p>
<h3 id="nn.functional">👀nn.functional</h3>
<p>nn中的层，一类是继承了nn.Module，其命名一般为nn.Xxx(第一个是大写)，如nn.Linear、nn.Conv2d、nn.CrossEntropyLoss等。另一类是nn.functional中的函数，其名称一般为nn.funtional.xxx，如nn.funtional.linear、nn.funtional.conv2d、nn.funtional.cross_entropy等。从功能来说两者相当，基于nn.Moudle能实现的层，使用nn.funtional也可实现，反之亦然，而且性能方面两者也没有太大差异。不过在具体使用时，两者还是有区别，主要区别如下：</p>
<ul>
<li><p>nn.Xxx继承于nn.Module，nn.Xxx需要先实例化并传入参数，然后以函数调用的方式调用实例化的对象并传入输入数据。它能够很好地与nn.Sequential结合使用，而nn.functional.xxx无法与nn.Sequential结合使用。</p></li>
<li><p>nn.Xxx不需要自己定义和管理weight、bias参数；而nn.functional.xxx需要自己定义weight、bias参数，每次调用的时候都需要手动传入weight、bias等参数，不利于代码复用。</p></li>
<li><p>Dropout操作在训练和测试阶段是有区别的，使用nn.Xxx方式定义Dropout，在调用model.eval()之后，自动实现状态的转换，而使用nn.functional.xxx却无此功能。</p></li>
</ul>
<p>总的来说，两种功能都是相同的，但PyTorch官方推荐：具有学习参数的(例如Conv2d、Linear、Batch_norm等)采用nn.Xxx方式。没有学习参数的(例如maxpool、loss func、activation func等)根据个人选择使用nn.Xxx或者nn.functional.xxx方式。</p>
<h3 id="优化器">👀优化器</h3>
<p><strong>（1）优化器一般步骤</strong></p>
<p>PyTorch常用的优化方法都封装在torch.optim里面，其设计很灵活，可以扩展为自定义的优化方法。所有的优化方法都是继承了基类optim.Optimizer，并实现了自己的优化步骤。最常用的优化算法就是梯度下降法及其各种变种，这类优化算法通过使用参数的梯度值更新参数。使用优化器的一般步骤为：</p>
<ul>
<li><p><strong>建立优化器实例</strong>：导入optim模块，实例化SGD（以随机梯度下降法为例）优化器。</p></li>
<li><p><strong>向前传播</strong>：把输入数据传入神经网络Net实例化对象model中，自动执行forward函数，得到out输出值（out = model(img)），然后用out与标记label计算损失值loss。loss = criterion(out, label)</p></li>
<li><p><strong>清空梯度</strong>：缺省情况梯度是累加的，在梯度反向传播前，先需把梯度清零。optimizer.zero_grad()</p></li>
<li><p><strong>反向传播</strong>：基于损失值，把梯度进行反向传播。loss.backward()</p></li>
<li><p><strong>更新参数</strong>：基于当前梯度（存储在参数的.grad属性中）更新参数。optimizer.step()</p></li>
</ul>
<blockquote>
<p>其中向前传播、清空梯度、反向传播、更新参数是在训练模型的for循环中。</p>
</blockquote>
<p>动态修改学习率参数：可以通过修改参数optimizer.params_groups或新建optimizer。新建optimizer比较简单，optimizer十分轻量级，所以开销很小。但是新的优化器会初始化动量等状态信息，这对于使用动量的优化器（momentum参数的sgd）可能会造成收敛中的震荡。</p>
<blockquote>
<p>optimizer.param_groups：长度1的list；</p>
<p>optimizer.param_groups[0]：长度为6的字典，包括权重参数、lr、momentum等参数。</p>
</blockquote>
<p><strong>（2）优化器种类</strong></p>
<p>优化器的方法主要可以分为两大类，一大类方法是SGD及其改进（加Momentum），另外一大类是逐参数适应学习率方法，包括AdaGrad、RMSProp、Adam等。<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_36589234/article/details/89330342">参考链接①</a>、<a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_62965652/article/details/136969277">参考链接②</a>、<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_42109740/article/details/105401197">参考链接③</a></p>
<ul>
<li><p>SGD(Stochastic Gradient Descent, 随机梯度下降)</p></li>
<li><p>SGD+Momentum动量法</p></li>
<li><p>NAG(Nesterov Accelerated Gradient)</p></li>
<li><p>Adagrad自适应学习率优化算法</p></li>
<li><p>RMSProp自适应学习率优化算法</p></li>
<li><p>Adam自适应学习率优化算法，本质上是将动量(Momentum)和RMSprop两种思想结合到一种算法中</p></li>
</ul>
<h2 id="pytorch数据处理工具箱">⛄PyTorch数据处理工具箱</h2>
<h3 id="utils.data和torchvision">👀utils.data和Torchvision</h3>
<p>数据下载和预处理是机器学习、深度学习实际项目中耗时又重要的任务，尤其是数据预处理，关系到数据质量和模型性能，要占据项目的大部分时间。PyTorch为此提供了专门的数据下载、数据处理包，使用这些包可极大地提高开发效率及数据质量。PyTorch涉及数据处理（数据装载、数据预处理、数据增强等）的主要工具包及相互关系如下图所示。</p>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pkt5JlF"><img src="/img/loading1.gif" data-original="https://s21.ax1x.com/2024/06/08/pkt5JlF.jpg" alt="pkt5JlF.jpg" /></a></p>
<p>torch.utils.data工具包，它包括以下4个类。</p>
<ul>
<li><p>Dataset：是一个抽象类，其他数据集需要继承这个类，并且覆写其中的两个方法(getitem、len)。</p></li>
<li><p>DataLoader：定义一个新的迭代器，实现批量(batch)读取，打乱数据(shuffle)并提供并行加速等功能。</p></li>
<li><p>random_split：把数据集随机拆分为给定长度的非重叠的新数据集。</p></li>
<li><p>*sampler：多种采样函数。</p></li>
</ul>
<p>Torchvision是PyTorch的一个视觉处理工具包，独立于PyTorch。它包括4个类，主要功能如下(<a target="_blank" rel="noopener" href="https://blog.csdn.net/xw555666/article/details/136068651">参考博客①</a>)：</p>
<ul>
<li><p>datasets：提供常用的数据集加载，设计上都是继承自torch.utils.data.Dataset，主要包括MMIST、CIFAR10/100、Fashion-MNIST、ImageNet、STL10和COCO等。</p></li>
<li><p>models：提供深度学习中各种经典的网络结构以及训练好的模型（如果选择pretrained=True），包括AlexNet、VGG系列、ResNet系列、Inception系列等。</p></li>
<li><p>transforms：常用的数据预处理操作，主要包括对Tensor及PIL Image对象的操作。</p></li>
<li><p>utils：含两个函数，一个是make_grid，它能将多张图片拼接在一个网格中；另一个是save_img，它能将Tensor保存成图片。</p></li>
</ul>
<h3 id="tensorboardx可视化工具">👀tensorboardX可视化工具</h3>
<p>Tensorboard是Google TensorFlow的可视化工具，它可以记录训练数据、评估数据、网络结构、图像等，并且可以在web上展示，对于观察神经网络训练的过程非常有帮助。PyTorch可以采用tensorboard_logger、visdom等可视化工具，但这些方法比较复杂或不够友好。为解决该问题，人们推出了可用于PyTorch可视化的新的更强大的工具——tensorboardX。tensorboardX功能很强大，支持scalar、image、figure、histogram、audio、text、graph、onnx_graph、embedding、pr_curve and videosummaries等可视化方式。<a target="_blank" rel="noopener" href="https://blog.csdn.net/bigbennyguo/article/details/87956434">参考博客①</a>、<a target="_blank" rel="noopener" href="https://blog.csdn.net/HowieXue/article/details/105412155">参考博客②</a></p>
<p>（1）导入tensorboardX，实例化SummaryWriter类，指明记录日志路径等信息</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorboardX <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="comment"># 实例化SummaryWriter，并指明日志存放路径。在当前目录没有logs目录将自动创建。</span></span><br><span class="line">writer = SummaryWriter(log_dir=<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line"><span class="comment"># 调用实例</span></span><br><span class="line">writer.add_xxx()</span><br><span class="line"><span class="comment"># 关闭writer</span></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<p>（2）调用相应的API接口</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">writer.add_xxx(tag-name, <span class="built_in">object</span>, iteration-number)</span><br><span class="line"><span class="comment"># 即add_xxx(标签，记录的对象，迭代次数)</span></span><br></pre></td></tr></table></figure>
<p>（3）启动tensorboard服务</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensorboard --logdir=logs --port <span class="number">6006</span></span><br><span class="line"><span class="comment"># 如果是Windows环境，要注意路径解析，如</span></span><br><span class="line"><span class="comment"># tensorboard --logdir=r&#x27;D:\myboard\test\logs&#x27; --port 6006</span></span><br></pre></td></tr></table></figure>
<p>（4）web展示</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">http://服务器IP或名称:<span class="number">6006</span> <span class="comment"># 如果是本机，服务器名称可以使用localhost</span></span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://heartlovelife.github.io">Xiaotangsmiles</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://heartlovelife.github.io/2024/06/10/DL-PyTorch-Basic/">https://heartlovelife.github.io/2024/06/10/DL-PyTorch-Basic/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://heartlovelife.github.io" target="_blank">江湖是你画中亦是你</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/PyTorch/">PyTorch</a></div><div class="post_share"><div class="social-share" data-image="https://s2.loli.net/2024/06/10/KdBQXSmlCgFOWR1.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/gh/overtrue/share.js@master/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/loading1.gif" data-original="/img/wechat.jpg" alt="wechat(微信)"/></a><div class="post-qr-code-desc">wechat(微信)</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/loading1.gif" data-original="/img/alipay.jpg" alt="alipay(支付宝)"/></a><div class="post-qr-code-desc">alipay(支付宝)</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/08/25/Python-GUI-Design/"><img class="prev-cover" src="/img/loading1.gif" data-original="https://s2.loli.net/2024/08/25/u7qX6PLeFUzxHvV.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">GUI编程之PyQt5入门详解（01）</div></div></a></div><div class="next-post pull-right"><a href="/2024/01/20/RS-LST-Method/"><img class="next-cover" src="/img/loading1.gif" data-original="https://s2.loli.net/2024/05/05/SHmUYgArEMzfLph.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">辐射传输基础理论详解与LST反演方法</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/loading1.gif" data-original="/img/touxiang.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Xiaotangsmiles</div><div class="author-info__description">我们总以为来日方长，却忘了世事无常</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">27</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">27</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">11</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/HeartLoveLife"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/HeartLoveLife" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/twg666" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=491037927&amp;website=www.oicqzone.com" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:xiaotangsmiles@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">我有我自己的太阳、月亮和星星，我有一个完全属于我自己的小世界。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">⛄前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pytorch%E5%9F%BA%E7%A1%80"><span class="toc-number">2.</span> <span class="toc-text">⛄PyTorch基础</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#numpy%E5%9F%BA%E7%A1%80"><span class="toc-number">2.1.</span> <span class="toc-text">👀Numpy基础</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#numpy%E6%95%B0%E7%BB%84"><span class="toc-number">2.1.1.</span> <span class="toc-text">👁Numpy数组</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E5%85%83%E7%B4%A0"><span class="toc-number">2.1.2.</span> <span class="toc-text">👁获取元素</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#numpy%E7%9A%84%E7%AE%97%E6%9C%AF%E8%BF%90%E7%AE%97"><span class="toc-number">2.1.3.</span> <span class="toc-text">👁Numpy的算术运算</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E7%BB%84%E5%8F%98%E5%BD%A2"><span class="toc-number">2.1.4.</span> <span class="toc-text">👁数组变形</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%B9%E9%87%8F%E5%A4%84%E7%90%86"><span class="toc-number">2.1.5.</span> <span class="toc-text">👁批量处理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%9A%E7%94%A8%E5%87%BD%E6%95%B0"><span class="toc-number">2.1.6.</span> <span class="toc-text">👁通用函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B9%BF%E6%92%AD%E6%9C%BA%E5%88%B6"><span class="toc-number">2.1.7.</span> <span class="toc-text">👁广播机制</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pytorch%E5%9F%BA%E7%A1%80-1"><span class="toc-number">2.2.</span> <span class="toc-text">👀PyTorch基础</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#tensor%E6%A6%82%E8%BF%B0"><span class="toc-number">2.2.1.</span> <span class="toc-text">👁Tensor概述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E5%BB%BAtensor"><span class="toc-number">2.2.2.</span> <span class="toc-text">👁创建Tensor</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9tensor%E5%BD%A2%E7%8A%B6"><span class="toc-number">2.2.3.</span> <span class="toc-text">👁修改Tensor形状</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95%E6%93%8D%E4%BD%9C"><span class="toc-number">2.2.4.</span> <span class="toc-text">👁索引操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B9%BF%E6%92%AD%E6%9C%BA%E5%88%B6-1"><span class="toc-number">2.2.5.</span> <span class="toc-text">👁广播机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%90%E5%85%83%E7%B4%A0%E6%93%8D%E4%BD%9C"><span class="toc-number">2.2.6.</span> <span class="toc-text">👁逐元素操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BD%92%E5%B9%B6%E6%93%8D%E4%BD%9C"><span class="toc-number">2.2.7.</span> <span class="toc-text">👁归并操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AF%94%E8%BE%83%E6%93%8D%E4%BD%9C"><span class="toc-number">2.2.8.</span> <span class="toc-text">👁比较操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E6%93%8D%E4%BD%9C"><span class="toc-number">2.2.9.</span> <span class="toc-text">👁矩阵操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#pytorch%E4%B8%8Enumpy%E6%AF%94%E8%BE%83"><span class="toc-number">2.2.10.</span> <span class="toc-text">👁PyTorch与Numpy比较</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tensor%E4%B8%8Eautograd"><span class="toc-number">2.3.</span> <span class="toc-text">👀Tensor与Autograd</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pytorch%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B7%A5%E5%85%B7%E7%AE%B1"><span class="toc-number">3.</span> <span class="toc-text">⛄PyTorch神经网络工具箱</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#nn.module"><span class="toc-number">3.1.</span> <span class="toc-text">👀nn.Module</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#nn.functional"><span class="toc-number">3.2.</span> <span class="toc-text">👀nn.functional</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">3.3.</span> <span class="toc-text">👀优化器</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pytorch%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%B7%A5%E5%85%B7%E7%AE%B1"><span class="toc-number">4.</span> <span class="toc-text">⛄PyTorch数据处理工具箱</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#utils.data%E5%92%8Ctorchvision"><span class="toc-number">4.1.</span> <span class="toc-text">👀utils.data和Torchvision</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tensorboardx%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7"><span class="toc-number">4.2.</span> <span class="toc-text">👀tensorboardX可视化工具</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/11/16/RS-CSink-Method/" title="碳汇估算方法概述及基于RS的碳汇估算模型详述【20241116】"><img src="/img/loading1.gif" data-original="https://s2.loli.net/2024/11/16/2lwpstimg6HCTod.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="碳汇估算方法概述及基于RS的碳汇估算模型详述【20241116】"/></a><div class="content"><a class="title" href="/2024/11/16/RS-CSink-Method/" title="碳汇估算方法概述及基于RS的碳汇估算模型详述【20241116】">碳汇估算方法概述及基于RS的碳汇估算模型详述【20241116】</a><time datetime="2024-11-16T12:20:20.000Z" title="发表于 2024-11-16 20:20:20">2024-11-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/10/28/Py6S-Model-Use/" title="常见大气校正模型及6S模型安装部署【20241028】"><img src="/img/loading1.gif" data-original="https://s2.loli.net/2024/10/28/mOhXvaL3YTEdMo9.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="常见大气校正模型及6S模型安装部署【20241028】"/></a><div class="content"><a class="title" href="/2024/10/28/Py6S-Model-Use/" title="常见大气校正模型及6S模型安装部署【20241028】">常见大气校正模型及6S模型安装部署【20241028】</a><time datetime="2024-10-28T12:13:14.000Z" title="发表于 2024-10-28 20:13:14">2024-10-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/09/23/Matlab-GUI-Design/" title="GUI编程之MATLAB入门详解（01）"><img src="/img/loading1.gif" data-original="https://s2.loli.net/2024/08/25/u7qX6PLeFUzxHvV.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="GUI编程之MATLAB入门详解（01）"/></a><div class="content"><a class="title" href="/2024/09/23/Matlab-GUI-Design/" title="GUI编程之MATLAB入门详解（01）">GUI编程之MATLAB入门详解（01）</a><time datetime="2024-09-23T14:22:08.000Z" title="发表于 2024-09-23 22:22:08">2024-09-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/09/15/RS-data-dowmload/" title="MODIS/Landsat/Sentinel下载教程详解【常用网站及方法枚举】"><img src="/img/loading1.gif" data-original="https://s2.loli.net/2024/09/15/xhwoIklLQq2mFp8.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="MODIS/Landsat/Sentinel下载教程详解【常用网站及方法枚举】"/></a><div class="content"><a class="title" href="/2024/09/15/RS-data-dowmload/" title="MODIS/Landsat/Sentinel下载教程详解【常用网站及方法枚举】">MODIS/Landsat/Sentinel下载教程详解【常用网站及方法枚举】</a><time datetime="2024-09-15T03:24:52.000Z" title="发表于 2024-09-15 11:24:52">2024-09-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/08/25/Python-GUI-Design/" title="GUI编程之PyQt5入门详解（01）"><img src="/img/loading1.gif" data-original="https://s2.loli.net/2024/08/25/u7qX6PLeFUzxHvV.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="GUI编程之PyQt5入门详解（01）"/></a><div class="content"><a class="title" href="/2024/08/25/Python-GUI-Design/" title="GUI编程之PyQt5入门详解（01）">GUI编程之PyQt5入门详解（01）</a><time datetime="2024-08-25T12:36:36.000Z" title="发表于 2024-08-25 20:36:36">2024-08-25</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2024 By Xiaotangsmiles</div><div class="framework-info"><span>地址 </span><a href="https://heartlovelife.github.io">HeartLoveLife</a><span class="footer-separator">|</span><span>邮箱 </span><a href="javascript:void(0);">xiaotangsmiles@163.com</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://heartlovelife.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(r){r.imageLazyLoadSetting.processImages=t;var e=r.imageLazyLoadSetting.isSPA,n=r.imageLazyLoadSetting.preloadRatio||1,c=a();function a(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(){e&&(c=a());for(var t,o=0;o<c.length;o++)0<=(t=(t=c[o]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(r.innerHeight*n||document.documentElement.clientHeight*n)&&function(){var t,e,n,a,i=c[o];e=function(){c=c.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(n=new Image,a=t.getAttribute("data-original"),n.onload=function(){t.src=a,t.removeAttribute("data-original"),e&&e()},t.src!==a&&(n.src=a))}()}function i(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",i),r.addEventListener("resize",i),r.addEventListener("orientationchange",i)}(this);</script></body></html>